{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c32e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib as jb\n",
    "from glob import glob\n",
    "from typing import Union, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "from tqdm import tqdm\n",
    "import joblib as jb\n",
    "# the following import is required for training to be robust to truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a50f0aa",
   "metadata": {},
   "source": [
    "## Re-train ResNet18 on DeepFashion Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f307a31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = './data/img'\n",
    "RESIZE = (256, 256)\n",
    "CROP = (224, 224)\n",
    "BATCHSIZE = 64\n",
    "VALID = 0.1\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e1a3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pre-trained ResNet18 \n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "# define a new output layer to match the number of classes in the dataset\n",
    "outputs = len(glob(DATADIR + '/*')) # the number of classes in the data folder\n",
    "inputs = resnet18.fc.in_features # extract the number of inputs from the final layer\n",
    "output_layer = nn.Linear(inputs, outputs) \n",
    "\n",
    "# freeze all but the avgpool and classifer layers \n",
    "for name, param in resnet18.named_parameters():\n",
    "    if 'fc' not in name and 'avgpool' not in name:\n",
    "        param.requires_grad=False\n",
    "\n",
    "# replace the classifier with the new output layer        \n",
    "resnet18.fc = output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d76b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to perform any augmentation other than resizing \n",
    "imageTransformations = transforms.Compose([ \n",
    "        transforms.Resize(RESIZE),\n",
    "        transforms.CenterCrop(CROP),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# load training and validation images\n",
    "fashionDataset = ImageFolder(DATADIR, transform=imageTransformations)\n",
    "\n",
    "# reserve 10% of images for validation purposes\n",
    "n_val = int(np.floor(VALID * len(fashionDataset)))\n",
    "n_train = len(fashionDataset) - n_val\n",
    "trainSet, validSet = random_split(fashionDataset, [n_train, n_val])\n",
    "\n",
    "# define the data loaders\n",
    "trainFashion = DataLoader(trainSet, batch_size=BATCHSIZE)\n",
    "validFashion = DataLoader(validSet , batch_size=BATCHSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c93965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c3b9332",
   "metadata": {},
   "source": [
    "## Image to Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26b4d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the last fc connected layer and expose the avgpool layer\n",
    "img2vec = nn.Sequential(*(list(resnet18.children())[:-1]))\n",
    "\n",
    "\n",
    "\n",
    "# sanity check\n",
    "print(img2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457865f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ouput_embeddings(\n",
    "        model: Union[nn.Sequential, nn.Module],\n",
    "        data: Union[torch.Tensor, np.ndarray],\n",
    "        device: torch.device\n",
    "    ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(data, torch.Tensor):\n",
    "        data = torch.from_numpy(data, dtype=torch.float32)\n",
    "        \n",
    "    if len(data.size()) == 3:\n",
    "        data = data.unsqueeze(0)\n",
    "    \n",
    "    outputs = _ouput_embeddings(model, data, device)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "\n",
    "def _ouput_embeddings(\n",
    "        model: Union[nn.Sequential, nn.Module],\n",
    "        data: Union[torch.Tensor, np.ndarray],\n",
    "        device: torch.device\n",
    "    ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    # pre inference\n",
    "    with torch.no_grad():\n",
    "        # turn off dropout/batch norm\n",
    "        model.eval() \n",
    "        # ensure the model and the images are on the same device\n",
    "        model, data = model.to(device), data.to(device)\n",
    "        # pass the batch through the model and save the outputs\n",
    "        outputs = model(data) \n",
    "\n",
    "    # clear GPU memory if working with GPU\n",
    "    model, data = model.to(\"cpu\"), data.to(\"cpu\")\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c16cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.c_[np.array([1,2,3]), np.array([4,5,6])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc60273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "embeddings = {}\n",
    "for batch, _ in tqdm(fashionLoader):\n",
    "    \n",
    "    outputs = output_embeddings(img2vec, batch, DEVICE)\n",
    "    temp = {img: out for img, out in zip(batch, outputs)}\n",
    "    embeddings.update(temp)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d98995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90ad634",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dict(zip(batch, embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065296b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {*batch, *embeddings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1aff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(a)\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6307435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5987bb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a76130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jb.dump(embeddings, 'deep_fashion_batched_embeddings.pkl', compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dff490",
   "metadata": {},
   "outputs": [],
   "source": [
    "jb.dump(embeddings, 'deep_fashion_batched_embeddings_backup.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2565b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = jb.load('deep_fashion_batched_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae8d5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92d5b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {image: None for image in batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d0315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5aba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the first plane\n",
    "first_plane = a[0]\n",
    "\n",
    "image = first_plane.permute(1, 2, 0)\n",
    "    \n",
    "# avoid clipping\n",
    "image -= image.min()\n",
    "image /= image.max()\n",
    "\n",
    "plt.axis('off')\n",
    "#plt.title(f'{idx_to_class[class_idx[0].item()]}')\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117edeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb71fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d76b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
